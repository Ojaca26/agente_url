import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

# --- Configuraci√≥n de la P√°gina de Streamlit ---
st.set_page_config(page_title="Agente Web Scraper IA", page_icon="ü§ñ")
st.title("ü§ñ Agente IA Web Scraper")
st.caption("Introduce la URL de un sitio web para extraer y ESTRUCTURAR su contenido.")

# --- Configuraci√≥n del Modelo de Lenguaje (LLM) ---
# ‚ö†Ô∏è Requiere librer√≠as actualizadas:
# pip install -U google-generativeai langchain-google-genai

try:
    llm = ChatGoogleGenerativeAI(
        model="models/gemini-1.5-pro",  # ‚úÖ usa prefijo "models/" (obligatorio en v1)
        google_api_key=st.secrets["GOOGLE_API_KEY"]
    )
    st.success("‚úÖ Conectado al modelo Gemini correctamente.")
except Exception as e:
    st.error(f"‚ùå Error al configurar el modelo de IA: {e}")
    st.stop()


# --- Funciones de Web Scraping ---

def es_valido(url):
    """Verifica si una URL es v√°lida."""
    parsed = urlparse(url)
    esquema_valido = bool(parsed.netloc) and bool(parsed.scheme)
    extensiones_excluidas = ['.pdf', '.jpg', '.png', '.zip', '.docx', '.gif', '.mp3', '.mp4']
    no_es_archivo = not any(parsed.path.lower().endswith(ext) for ext in extensiones_excluidas)
    return esquema_valido and no_es_archivo


def obtener_enlaces_pagina(url):
    """Obtiene todos los enlaces v√°lidos de una p√°gina web."""
    urls = set()
    try:
        respuesta = requests.get(url, timeout=10, headers={'User-Agent': 'Mozilla/5.0'})
        respuesta.raise_for_status()
        sopa = BeautifulSoup(respuesta.content, 'html.parser')
        dominio_base = f"{urlparse(url).scheme}://{urlparse(url).netloc}"

        for link in sopa.find_all('a', href=True):
            href = link['href']
            url_absoluta = urljoin(dominio_base, href)
            if urlparse(url_absoluta).netloc == urlparse(dominio_base).netloc and es_valido(url_absoluta):
                urls.add(url_absoluta)
    except Exception as e:
        st.warning(f"No se pudo acceder a {url}: {e}")
    return urls


def extraer_texto(url):
    """Extrae el texto principal de una p√°gina web."""
    try:
        respuesta = requests.get(url, timeout=10, headers={'User-Agent': 'Mozilla/5.0'})
        respuesta.raise_for_status()
        sopa = BeautifulSoup(respuesta.content, 'html.parser')
        for script_o_estilo in sopa(["script", "style"]):
            script_o_estilo.decompose()
        texto = " ".join(t.strip() for t in sopa.stripped_strings)
        return texto
    except Exception as e:
        st.warning(f"No se pudo extraer texto de {url}: {e}")
        return ""


# --- L√≥gica del Agente con LangChain (Versi√≥n que no resume) ---

def analizar_y_estructurar_contenido(texto_pagina, url):
    """
    Utiliza el LLM para ESTRUCTURAR todo el contenido de una p√°gina, SIN RESUMIR.
    """
    if not texto_pagina:
        return "No se encontr√≥ contenido textual para analizar."

    plantilla_nueva = """
    Eres un asistente de IA experto en analizar y documentar contenido web de forma exhaustiva.
    Tu tarea es procesar el texto extra√≠do de la URL '{url}' y presentarlo de forma completa y bien organizada. 
    **No debes resumir, acortar ni omitir informaci√≥n relevante.**

    Realiza las siguientes acciones con el texto proporcionado:
    1. Genera un t√≠tulo descriptivo que refleje el contenido principal de la p√°gina.
    2. Identifica las diferentes secciones o temas principales.
    3. Para cada secci√≥n, presenta TODA la informaci√≥n correspondiente usando subt√≠tulos Markdown (##).
    4. Devuelve una transcripci√≥n estructurada fiel al contenido original.

    Contenido de la p√°gina:
    ---
    {texto}
    ---
    """

    prompt = ChatPromptTemplate.from_template(plantilla_nueva)
    cadena = prompt | llm | StrOutputParser()
    texto_limitado = texto_pagina[:30000]

    try:
        return cadena.invoke({"texto": texto_limitado, "url": url})
    except Exception as e:
        st.warning(f"Error procesando {url}: {e}")
        return "‚ö†Ô∏è No se pudo estructurar el contenido de esta p√°gina."


# --- Interfaz de Usuario de Streamlit ---

url_usuario = st.text_input("üîó Introduce la URL del sitio web a escanear:", placeholder="https://ejemplo.com")

if st.button("üöÄ Iniciar Escaneo"):
    if url_usuario and es_valido(url_usuario):
        with st.spinner('üîç Buscando enlaces en la p√°gina principal...'):
            enlaces_a_visitar = list(obtener_enlaces_pagina(url_usuario))
            enlaces_a_visitar.insert(0, url_usuario)
            visitados = set()
            contenido_final = f"# Reporte de Contenido Web: {urlparse(url_usuario).netloc}\n\n"

        max_paginas = st.slider("N√∫mero m√°ximo de p√°ginas a analizar:", 1, 20, 5)
        barra_progreso = st.progress(0)

        for i, enlace in enumerate(enlaces_a_visitar[:max_paginas]):
            if enlace not in visitados:
                visitados.add(enlace)
                with st.spinner(f"Analizando p√°gina {i+1}/{len(enlaces_a_visitar[:max_paginas])}: {enlace}"):
                    texto = extraer_texto(enlace)
                    if texto:
                        contenido_estructurado = analizar_y_estructurar_contenido(texto, enlace)
                        contenido_final += f"## P√°gina Analizada: {enlace}\n\n{contenido_estructurado}\n\n---\n\n"
                        st.write(f"‚úÖ Contenido estructurado para: {enlace}")
                    else:
                        st.write(f"‚ö†Ô∏è No se pudo extraer texto de: {enlace}")

                barra_progreso.progress((i + 1) / max_paginas)

        st.success("¬°Escaneo completado!")
        st.markdown("---")
        st.header("üìÑ Contenido Estructurado del Sitio Web")
        st.markdown(contenido_final)
    else:
        st.error("Por favor, introduce una URL v√°lida.")
