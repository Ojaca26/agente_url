import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import google.generativeai as genai
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
import os

# --- Configuraci√≥n de la p√°gina ---
st.set_page_config(page_title="Agente URL ‚Üí Prompt Bot Asesor", page_icon="ü§ñ")
st.title("ü§ñ Agente IA ‚Äì Generador de Prompt para Bot Asesor")
st.caption("Escanea una URL, estructura su contenido y genera un prompt profesional para un chatbot de atenci√≥n al cliente.")

# --- Inicializar API de Gemini ---
try:
    api_key = st.secrets["GOOGLE_API_KEY"]
    os.environ["GOOGLE_API_KEY"] = api_key
    genai.configure(api_key=api_key)

    st.success("‚úÖ Conectado correctamente al modelo Gemini.")
    llm = ChatGoogleGenerativeAI(
        model="models/gemini-2.5-pro",
        google_api_key=api_key
    )
except Exception as e:
    st.error(f"‚ùå Error al configurar la API de Google: {e}")
    st.stop()


# --- Funciones auxiliares ---
def es_valido(url):
    parsed = urlparse(url)
    esquema_valido = bool(parsed.netloc) and bool(parsed.scheme)
    extensiones_excluidas = ['.pdf', '.jpg', '.png', '.zip', '.docx', '.gif', '.mp3', '.mp4']
    no_es_archivo = not any(parsed.path.lower().endswith(ext) for ext in extensiones_excluidas)
    return esquema_valido and no_es_archivo


def obtener_enlaces_pagina(url):
    urls = set()
    try:
        r = requests.get(url, timeout=10, headers={'User-Agent': 'Mozilla/5.0'})
        r.raise_for_status()
        sopa = BeautifulSoup(r.content, 'html.parser')
        base = f"{urlparse(url).scheme}://{urlparse(url).netloc}"
        for link in sopa.find_all('a', href=True):
            href = link['href']
            absoluta = urljoin(base, href)
            if urlparse(absoluta).netloc == urlparse(base).netloc and es_valido(absoluta):
                urls.add(absoluta)
    except Exception as e:
        st.warning(f"No se pudo acceder a {url}: {e}")
    return urls


def extraer_texto(url):
    try:
        r = requests.get(url, timeout=10, headers={'User-Agent': 'Mozilla/5.0'})
        r.raise_for_status()
        sopa = BeautifulSoup(r.content, 'html.parser')
        for s in sopa(["script", "style"]):
            s.decompose()
        texto = " ".join(t.strip() for t in sopa.stripped_strings)
        return texto
    except Exception as e:
        st.warning(f"No se pudo extraer texto de {url}: {e}")
        return ""


# --- Paso 1: estructurar contenido ---
def analizar_y_estructurar_contenido(texto_pagina, url):
    plantilla = """
    Eres un asistente de IA experto en analizar y documentar contenido web.
    Procesa el texto de la URL '{url}' y pres√©ntalo completo, organizado por secciones.
    Usa Markdown (## para secciones). No resumas ni omitas detalles.

    Contenido:
    ---
    {texto}
    ---
    """
    prompt = ChatPromptTemplate.from_template(plantilla)
    cadena = prompt | llm | StrOutputParser()
    texto_limitado = texto_pagina[:30000]
    try:
        return cadena.invoke({"texto": texto_limitado, "url": url})
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Error procesando {url}: {e}")
        return ""


# --- Paso 2: generar prompt final ---
def generar_prompt_bot(nombre_empresa, link_empresa, contenido_total, guia_base):
    plantilla_bot = f"""
Eres un analista de IA especializado en crear prompts de asistentes virtuales.

Usa toda la informaci√≥n proporcionada del sitio web de la empresa **{nombre_empresa}** para generar un prompt completo y profesional para un **bot asesor y de servicio al cliente**. 

Debes inspirarte en la siguiente gu√≠a estructural y adaptarla al contexto real de la empresa:

---
### Gu√≠a estructural del prompt del bot:
{guia_base}
---

Ahora, con base en el contenido analizado y la estructura gu√≠a anterior, genera el **prompt final** personalizado para {nombre_empresa}.
Incluye:
- Descripci√≥n de la empresa y su prop√≥sito.
- Reglas del chatbot.
- Introducci√≥n y consentimiento.
- Men√∫ principal y subopciones.
- Flujo conversacional con respuestas naturales y humanas.
- Frases y tono gu√≠a.
- Cierre amable y proactivo.

Informaci√≥n base del sitio ({link_empresa}):
---
{contenido_total[:25000]}
---
"""
    prompt = ChatPromptTemplate.from_template(plantilla_bot)
    cadena = prompt | llm | StrOutputParser()
    try:
        return cadena.invoke({})
    except Exception as e:
        st.error(f"‚ö†Ô∏è Error generando el prompt del bot: {e}")
        return "Error al generar el prompt del bot."


# --- Entrada de usuario ---
url_usuario = st.text_input("üåê Introduce la URL del sitio web:", placeholder="https://ejemplo.com")
nombre_empresa = st.text_input("üè¢ Nombre de la empresa:", placeholder="Kravata")
archivo_prompt = "prompt_kravata.txt"

# --- Cargar gu√≠a base desde archivo ---
try:
    with open(archivo_prompt, "r", encoding="utf-8") as f:
        guia_base = f.read()
        st.success(f"üìÑ Gu√≠a base cargada desde '{archivo_prompt}' correctamente.")
except Exception as e:
    st.error(f"‚ö†Ô∏è No se pudo cargar la gu√≠a base '{archivo_prompt}': {e}")
    st.stop()

st.markdown("---")

# --- Ejecuci√≥n principal ---
if st.button("üöÄ Iniciar Escaneo y Generar Prompt"):
    if url_usuario and es_valido(url_usuario):
        with st.spinner('üîç Buscando enlaces en la p√°gina principal...'):
            enlaces = list(obtener_enlaces_pagina(url_usuario))
            enlaces.insert(0, url_usuario)
            visitados, contenido_final = set(), f"# Reporte: {urlparse(url_usuario).netloc}\n\n"

        max_paginas = st.slider("N√∫mero m√°ximo de p√°ginas a analizar:", 1, 15, 5)
        barra = st.progress(0)

        for i, enlace in enumerate(enlaces[:max_paginas]):
            if enlace not in visitados:
                visitados.add(enlace)
                with st.spinner(f"Analizando p√°gina {i+1}/{len(enlaces[:max_paginas])}: {enlace}"):
                    texto = extraer_texto(enlace)
                    if texto:
                        resultado = analizar_y_estructurar_contenido(texto, enlace)
                        contenido_final += f"## P√°gina: {enlace}\n\n{resultado}\n\n---\n\n"
                    barra.progress((i + 1) / max_paginas)

        st.success("‚úÖ Contenido estructurado correctamente")
        st.markdown("---")
        st.header("üìÑ Contenido Analizado del Sitio")
        st.markdown(contenido_final[:30000])

        # Generar prompt final
        st.header("üß† Generando Prompt Personalizado del Bot...")
        prompt_final = generar_prompt_bot(nombre_empresa, url_usuario, contenido_final, guia_base)

        st.success("üéØ Prompt generado exitosamente")
        st.text_area("üìò Prompt Final para el Bot", prompt_final, height=500)

        # Guardar como archivo .txt
        archivo_salida = f"Prompt_{nombre_empresa.replace(' ', '_')}.txt"
        with open(archivo_salida, "w", encoding="utf-8") as f:
            f.write(prompt_final)

        with open(archivo_salida, "rb") as file:
            st.download_button(
                label="üì• Descargar Prompt (.txt)",
                data=file,
                file_name=archivo_salida,
                mime="text/plain"
            )
    else:
        st.error("Por favor, introduce una URL v√°lida.")
